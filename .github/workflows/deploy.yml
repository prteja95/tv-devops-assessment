name: CI/CD Deploy to AWS ECS

on:
  push:
    branches: [main]

env:
  AWS_REGION: ${{ vars.AWS_REGION }}
  APP_REPO_NAME: ${{ vars.APP_REPO_NAME }}
  APP_CLUSTER_NAME: ${{ vars.APP_CLUSTER_NAME }}
  APP_IMAGE_TAG: ${{ vars.APP_IMAGE_TAG }}
  CONTAINER_PORT: ${{ vars.CONTAINER_PORT }}
  ALB_PORT: ${{ vars.ALB_PORT }}
  DESIRED_TASKS: ${{ vars.DESIRED_TASKS }}
  ALB_ALLOWED_CIDRS: ${{ vars.ALB_ALLOWED_CIDRS }}
  SG_EGRESS_CIDRS: ${{ vars.SG_EGRESS_CIDRS }}
  ECS_CPU: ${{ vars.ECS_CPU }}
  ECS_MEMORY: ${{ vars.ECS_MEMORY }}
  LOG_RETENTION_DAYS: ${{ vars.LOG_RETENTION_DAYS }}

jobs:
  deploy:
    runs-on: ubuntu-latest
    environment: production

    steps:
      # Step 1: Checkout code
      - name: Checkout repository
        uses: actions/checkout@v4

      # Step 2: Clean CIDR blocks
      - name: Clean CIDR blocks
        id: clean-cidr
        run: |
          echo "CUSTOM_VPC_CIDR=${VARS_CUSTOM_VPC_CIDR//[$'\t\r\n ']}" >> $GITHUB_ENV
          echo "PUBLIC_SUBNET_CIDR_A=${VARS_PUBLIC_SUBNET_CIDR_A//[$'\t\r\n ']}" >> $GITHUB_ENV
          echo "PUBLIC_SUBNET_CIDR_B=${VARS_PUBLIC_SUBNET_CIDR_B//[$'\t\r\n ']}" >> $GITHUB_ENV
          echo "PRIVATE_SUBNET_CIDR_A=${VARS_PRIVATE_SUBNET_CIDR_A//[$'\t\r\n ']}" >> $GITHUB_ENV
          echo "PRIVATE_SUBNET_CIDR_B=${VARS_PRIVATE_SUBNET_CIDR_B//[$'\t\r\n ']}" >> $GITHUB_ENV
        env:
          VARS_CUSTOM_VPC_CIDR: ${{ vars.CUSTOM_VPC_CIDR }}
          VARS_PUBLIC_SUBNET_CIDR_A: ${{ vars.PUBLIC_SUBNET_CIDR_A }}
          VARS_PUBLIC_SUBNET_CIDR_B: ${{ vars.PUBLIC_SUBNET_CIDR_B }}
          VARS_PRIVATE_SUBNET_CIDR_A: ${{ vars.PRIVATE_SUBNET_CIDR_A }}
          VARS_PRIVATE_SUBNET_CIDR_B: ${{ vars.PRIVATE_SUBNET_CIDR_B }}

      # Step 3: Configure AWS credentials
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # Step 4: Setup Node.js
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20.x
          cache: 'npm'
          cache-dependency-path: 'iac/package-lock.json'

      # Step 5: Install CDKTF
      - name: Install CDKTF
        run: npm install -g cdktf-cli@latest

      # Step 6: Install Terraform
      - name: Install Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: latest
          terraform_wrapper: false

      # Step 7: Comprehensive Pre-Deployment Cleanup
      - name: Cleanup Existing all AWS Resources from previous deployement
        continue-on-error: true
        run: |
          # 1. Clean up existing VPC and related resources
          echo "Checking for existing VPC..."
          EXISTING_VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=tv-devops-vpc" \
            --query "Vpcs[0].VpcId" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$EXISTING_VPC_ID" ]; then
            echo "Found existing VPC ($EXISTING_VPC_ID), cleaning up resources..."
            
            # 1a. Delete ALB and related resources
            echo "Cleaning up ALB resources..."
            ALB_ARN=$(aws elbv2 describe-load-balancers \
              --query "LoadBalancers[?VpcId=='$EXISTING_VPC_ID'].LoadBalancerArn" \
              --output text)
            if [ -n "$ALB_ARN" ]; then
              aws elbv2 delete-load-balancer --load-balancer-arn $ALB_ARN || echo "ALB deletion failed, continuing..."
              aws elbv2 wait load-balancers-deleted --load-balancer-arns $ALB_ARN || echo "ALB wait failed"
            fi
            
            # 1b. Delete NAT Gateways
            echo "Cleaning up NAT Gateways..."
            NAT_GW_IDS=$(aws ec2 describe-nat-gateways \
              --filter "Name=vpc-id,Values=$EXISTING_VPC_ID" \
              --query "NatGateways[].NatGatewayId" \
              --output text)
            for NAT_GW_ID in $NAT_GW_IDS; do
              aws ec2 delete-nat-gateway --nat-gateway-id $NAT_GW_ID || echo "Failed to delete NAT GW $NAT_GW_ID"
            done
            
            # 1c. Detach and delete Internet Gateways
            echo "Cleaning up Internet Gateways..."
            IGW_IDS=$(aws ec2 describe-internet-gateways \
              --filters "Name=attachment.vpc-id,Values=$EXISTING_VPC_ID" \
              --query "InternetGateways[].InternetGatewayId" \
              --output text)
            for IGW_ID in $IGW_IDS; do
              aws ec2 detach-internet-gateway --internet-gateway-id $IGW_ID --vpc-id $EXISTING_VPC_ID || echo "Detach failed"
              aws ec2 delete-internet-gateway --internet-gateway-id $IGW_ID || echo "Delete failed"
            done
            
            # 1d. Delete VPC (will fail if dependencies still exist)
            echo "Attempting to delete VPC..."
            aws ec2 delete-vpc --vpc-id $EXISTING_VPC_ID || echo "VPC deletion failed (may have remaining dependencies)"
          fi
          
          # 2. Clean up CloudWatch Log Groups
          echo "Cleaning up CloudWatch Log Groups..."
          aws logs delete-log-group \
            --log-group-name "/ecs/tv-devops-cluster" 2>/dev/null || echo "Log group not found or already deleted"
          
          # 3. Clean up ECR Repository
          echo "Cleaning up ECR Repository..."
          aws ecr delete-repository \
            --repository-name "tv-devops-app" \
            --force 2>/dev/null || echo "ECR repository not found or already deleted"
          
          # 4. Clean up any remaining unattached IGWs (VPC limit prevention)
          echo "Cleaning up unattached Internet Gateways..."
          UNATTACHED_IGWS=$(aws ec2 describe-internet-gateways \
            --filters "Name=attachment.vpc-id,Values=" \
            --query "InternetGateways[].InternetGatewayId" \
            --output text)
          for IGW_ID in $UNATTACHED_IGWS; do
            aws ec2 delete-internet-gateway --internet-gateway-id $IGW_ID || echo "Could not delete IGW $IGW_ID"
          done
          
          echo "Cleanup complete. Waiting for AWS resource propagation..."
          sleep 60
        env:
          AWS_REGION: ${{ env.AWS_REGION }}

      # Step 8: Wait for cleanup completion
      - name: Wait for resource cleanup
        run: sleep 160

      # Step 9: Deploy AWS Infrastructure
      - name: Deploy AWS Infrastructure
        run: |
          cd iac
          npm ci
          cdktf get
          cdktf deploy --auto-approve
        env:
          AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
          CUSTOM_VPC_CIDR: ${{ env.CUSTOM_VPC_CIDR }}
          PUBLIC_SUBNET_CIDR_A: ${{ env.PUBLIC_SUBNET_CIDR_A }}
          PUBLIC_SUBNET_CIDR_B: ${{ env.PUBLIC_SUBNET_CIDR_B }}
          PRIVATE_SUBNET_CIDR_A: ${{ env.PRIVATE_SUBNET_CIDR_A }}
          PRIVATE_SUBNET_CIDR_B: ${{ env.PRIVATE_SUBNET_CIDR_B }}

      # Step 10: Verify Network Connectivity
      - name: Verify ECR Access
        run: |
          aws ec2 describe-nat-gateways --region $AWS_REGION
          aws ecr describe-repositories --repository-names $APP_REPO_NAME --region $AWS_REGION

      # Step 11: Wait for infrastructure stabilization
      - name: Wait for infrastructure stabilization
        run: sleep 60

      # Step 12: Login to ECR
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      # Step 13: Build Docker image
      - name: Build Docker image
        run: |
          docker buildx build --platform linux/amd64 \
            -t ${{ steps.login-ecr.outputs.registry }}/${{ env.APP_REPO_NAME }}:${{ env.APP_IMAGE_TAG }} \
            ./app

      # Step 14: Push Docker image
      - name: Push Docker image
        run: |
          docker push ${{ steps.login-ecr.outputs.registry }}/${{ env.APP_REPO_NAME }}:${{ env.APP_IMAGE_TAG }}

      # Step 15: Force ECS deployment
      - name: Force ECS deployment
        run: |
          aws ecs update-service \
            --cluster ${{ env.APP_CLUSTER_NAME }} \
            --service ${{ env.APP_CLUSTER_NAME }}-service \
            --force-new-deployment \
            --region ${{ env.AWS_REGION }}
          aws ecs wait services-stable \
            --cluster ${{ env.APP_CLUSTER_NAME }} \
            --services ${{ env.APP_CLUSTER_NAME }}-service \
            --region ${{ env.AWS_REGION }}
          sleep 120

      # Step 16: Get ALB DNS with robust error handling
      - name: Get ALB DNS
        id: alb-dns
        run: |
          echo "Attempting to get ALB DNS..."
          # Try multiple methods to get ALB DNS
          DNS_NAME=""
          
          # Method 1: Try terraform output
          if [ -d "iac" ]; then
            cd iac
            DNS_NAME=$(terraform output -raw albDnsName 2>/dev/null || echo "")
            cd ..
          fi
          
          # Method 2: Try AWS CLI if terraform failed
          if [ -z "$DNS_NAME" ]; then
            echo "Falling back to AWS CLI..."
            DNS_NAME=$(aws elbv2 describe-load-balancers \
              --region ${{ env.AWS_REGION }} \
              --names tv-devops-alb \
              --query 'LoadBalancers[0].DNSName' \
              --output text 2>/dev/null || echo "")
          fi
          
          # Method 3: Try generic describe if named lookup failed
          if [ -z "$DNS_NAME" ]; then
            echo "Falling back to generic ALB lookup..."
            DNS_NAME=$(aws elbv2 describe-load-balancers \
              --region ${{ env.AWS_REGION }} \
              --query 'LoadBalancers[?contains(LoadBalancerName,`tv-devops`)].DNSName' \
              --output text 2>/dev/null || echo "")
          fi
          
          if [ -z "$DNS_NAME" ]; then
            echo "ERROR: Could not retrieve ALB DNS name"
            exit 1
          fi
          
          echo "ALB DNS Name: $DNS_NAME"
          echo "ALB_DNS=$DNS_NAME" >> $GITHUB_ENV

      # Step 17: Test endpoint with enhanced retry logic
      - name: Test ALB endpoint
        run: |
          echo "Starting ALB health check..."
          MAX_RETRIES=15
          RETRY_DELAY=20
          HEALTH_ENDPOINT="http://${ALB_DNS}/health"
          
          for ((i=1; i<=$MAX_RETRIES; i++)); do
            echo "Attempt $i/$MAX_RETRIES: Testing $HEALTH_ENDPOINT"
            
            # Use curl with timeout and detailed error reporting
            if curl -sSf --connect-timeout 10 --max-time 15 "$HEALTH_ENDPOINT"; then
              echo "Health check passed successfully"
              exit 0
            else
              CURL_EXIT_CODE=$?
              echo "Health check failed (curl exit code: $CURL_EXIT_CODE)"
              
              # Additional debugging for common curl errors
              case $CURL_EXIT_CODE in
                6) echo "Error: Could not resolve host";;
                7) echo "Error: Failed to connect to host";;
                28) echo "Error: Connection timed out";;
                *) echo "Error: Unknown curl error";;
              esac
              
              if [ $i -lt $MAX_RETRIES ]; then
                echo "Waiting $RETRY_DELAY seconds before next attempt..."
                sleep $RETRY_DELAY
                
                # Optional: Verify ALB state
                echo "Current ALB state:"
                aws elbv2 describe-load-balancers \
                  --region ${{ env.AWS_REGION }} \
                  --names tv-devops-alb \
                  --query 'LoadBalancers[0].State.Code' \
                  --output text || echo "ALB status check failed"
              fi
            fi
          done
          
          echo "ALB health check failed after $MAX_RETRIES attempts"
          echo "Final ALB state:"
          aws elbv2 describe-load-balancers \
            --region ${{ env.AWS_REGION }} \
            --names tv-devops-alb \
            --query 'LoadBalancers[0]' \
            --output json
          exit 1
