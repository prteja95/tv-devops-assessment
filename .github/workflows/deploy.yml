name: CI/CD Deploy to AWS ECS

on:
  push:
    branches: [main]

env:
  AWS_REGION: ${{ vars.AWS_REGION }}
  APP_REPO_NAME: ${{ vars.APP_REPO_NAME }}
  APP_CLUSTER_NAME: ${{ vars.APP_CLUSTER_NAME }}
  APP_IMAGE_TAG: ${{ vars.APP_IMAGE_TAG }}
  CONTAINER_PORT: ${{ vars.CONTAINER_PORT }}
  ALB_PORT: ${{ vars.ALB_PORT }}
  DESIRED_TASKS: ${{ vars.DESIRED_TASKS }}
  ALB_ALLOWED_CIDRS: ${{ vars.ALB_ALLOWED_CIDRS }}
  SG_EGRESS_CIDRS: ${{ vars.SG_EGRESS_CIDRS }}
  ECS_CPU: ${{ vars.ECS_CPU }}
  ECS_MEMORY: ${{ vars.ECS_MEMORY }}
  LOG_RETENTION_DAYS: ${{ vars.LOG_RETENTION_DAYS }}

jobs:
  deploy:
    runs-on: ubuntu-latest
    environment: production

    steps:
      # Step 1: Checkout code
      - name: Checkout repository
        uses: actions/checkout@v4

      # Step 2: Clean CIDR blocks
      - name: Clean CIDR blocks
        id: clean-cidr
        run: |
          echo "CUSTOM_VPC_CIDR=${VARS_CUSTOM_VPC_CIDR//[$'\t\r\n ']}" >> $GITHUB_ENV
          echo "PUBLIC_SUBNET_CIDR_A=${VARS_PUBLIC_SUBNET_CIDR_A//[$'\t\r\n ']}" >> $GITHUB_ENV
          echo "PUBLIC_SUBNET_CIDR_B=${VARS_PUBLIC_SUBNET_CIDR_B//[$'\t\r\n ']}" >> $GITHUB_ENV
          echo "PRIVATE_SUBNET_CIDR_A=${VARS_PRIVATE_SUBNET_CIDR_A//[$'\t\r\n ']}" >> $GITHUB_ENV
          echo "PRIVATE_SUBNET_CIDR_B=${VARS_PRIVATE_SUBNET_CIDR_B//[$'\t\r\n ']}" >> $GITHUB_ENV
        env:
          VARS_CUSTOM_VPC_CIDR: ${{ vars.CUSTOM_VPC_CIDR }}
          VARS_PUBLIC_SUBNET_CIDR_A: ${{ vars.PUBLIC_SUBNET_CIDR_A }}
          VARS_PUBLIC_SUBNET_CIDR_B: ${{ vars.PUBLIC_SUBNET_CIDR_B }}
          VARS_PRIVATE_SUBNET_CIDR_A: ${{ vars.PRIVATE_SUBNET_CIDR_A }}
          VARS_PRIVATE_SUBNET_CIDR_B: ${{ vars.PRIVATE_SUBNET_CIDR_B }}

      # Step 3: Configure AWS credentials
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # Step 4: Setup Node.js
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20.x
          cache: 'npm'
          cache-dependency-path: 'iac/package-lock.json'

      # Step 5: Install CDKTF
      - name: Install CDKTF
        run: npm install -g cdktf-cli@latest

      # Step 6: Install Terraform
      - name: Install Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: latest
          terraform_wrapper: false

      # Step 7: Comprehensive Cleanup Before Deployment
      - name: Cleanup Existing AWS Resources
        continue-on-error: true
        run: |
          # 1. Clean up orphaned Internet Gateways
          echo "Cleaning up unattached Internet Gateways..."
          IGW_IDS=$(aws ec2 describe-internet-gateways \
            --filters "Name=attachment.vpc-id,Values=" \
            --query "InternetGateways[].InternetGatewayId" \
            --output text)
          
          for IGW_ID in $IGW_IDS; do
            echo "Deleting unattached IGW: $IGW_ID"
            aws ec2 delete-internet-gateway --internet-gateway-id $IGW_ID || echo "IGW deletion failed, continuing..."
          done

          # 2. Clean up ALB resources
          echo "Cleaning up ALB resources..."
          ALB_ARN=$(aws elbv2 describe-load-balancers \
            --names tv-devops-alb \
            --query 'LoadBalancers[0].LoadBalancerArn' \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$ALB_ARN" ]; then
            echo "Deleting ALB: $ALB_ARN"
            aws elbv2 delete-load-balancer --load-balancer-arn $ALB_ARN
            aws elbv2 wait load-balancers-deleted --load-balancer-arns $ALB_ARN
          fi

          # 3. Clean up NAT Gateways
          echo "Cleaning up NAT Gateways..."
          NAT_GW_IDS=$(aws ec2 describe-nat-gateways \
            --filter "Name=state,Values=failed,available" \
            --query "NatGateways[].NatGatewayId" \
            --output text)
          
          for NAT_GW_ID in $NAT_GW_IDS; do
            echo "Deleting NAT Gateway: $NAT_GW_ID"
            aws ec2 delete-nat-gateway --nat-gateway-id $NAT_GW_ID || echo "NAT GW deletion failed, continuing..."
          done

          # 4. Clean up VPC attachments
          echo "Cleaning up VPC attachments..."
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=tv-devops-vpc" \
            --query "Vpcs[0].VpcId" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$VPC_ID" ]; then
            # Detach IGWs
            ATTACHED_IGWS=$(aws ec2 describe-internet-gateways \
              --filters "Name=attachment.vpc-id,Values=$VPC_ID" \
              --query "InternetGateways[].InternetGatewayId" \
              --output text)
            
            for IGW_ID in $ATTACHED_IGWS; do
              echo "Detaching IGW $IGW_ID from VPC $VPC_ID"
              aws ec2 detach-internet-gateway \
                --internet-gateway-id $IGW_ID \
                --vpc-id $VPC_ID || echo "Detach failed, continuing..."
            done
          fi
        env:
          AWS_REGION: ${{ env.AWS_REGION }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      # Step 8: Wait for cleanup completion
      - name: Wait for resource cleanup
        run: sleep 160

      # Step 9: Deploy AWS Infrastructure
      - name: Deploy AWS Infrastructure
        run: |
          cd iac
          npm ci
          cdktf get
          cdktf deploy --auto-approve
        env:
          AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
          CUSTOM_VPC_CIDR: ${{ env.CUSTOM_VPC_CIDR }}
          PUBLIC_SUBNET_CIDR_A: ${{ env.PUBLIC_SUBNET_CIDR_A }}
          PUBLIC_SUBNET_CIDR_B: ${{ env.PUBLIC_SUBNET_CIDR_B }}
          PRIVATE_SUBNET_CIDR_A: ${{ env.PRIVATE_SUBNET_CIDR_A }}
          PRIVATE_SUBNET_CIDR_B: ${{ env.PRIVATE_SUBNET_CIDR_B }}

      # Step 10: Verify Network Connectivity
      - name: Verify ECR Access
        run: |
          aws ec2 describe-nat-gateways --region $AWS_REGION
          aws ecr describe-repositories --repository-names $APP_REPO_NAME --region $AWS_REGION

      # Step 11: Wait for infrastructure stabilization
      - name: Wait for infrastructure stabilization
        run: sleep 60

      # Step 12: Login to ECR
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      # Step 13: Build Docker image
      - name: Build Docker image
        run: |
          docker buildx build --platform linux/amd64 \
            -t ${{ steps.login-ecr.outputs.registry }}/${{ env.APP_REPO_NAME }}:${{ env.APP_IMAGE_TAG }} \
            ./app

      # Step 14: Push Docker image
      - name: Push Docker image
        run: |
          docker push ${{ steps.login-ecr.outputs.registry }}/${{ env.APP_REPO_NAME }}:${{ env.APP_IMAGE_TAG }}

      # Step 15: Force ECS deployment
      - name: Force ECS deployment
        run: |
          aws ecs update-service \
            --cluster ${{ env.APP_CLUSTER_NAME }} \
            --service ${{ env.APP_CLUSTER_NAME }}-service \
            --force-new-deployment \
            --region ${{ env.AWS_REGION }}
          aws ecs wait services-stable \
            --cluster ${{ env.APP_CLUSTER_NAME }} \
            --services ${{ env.APP_CLUSTER_NAME }}-service \
            --region ${{ env.AWS_REGION }}
          sleep 120

      # Step 16: Get ALB DNS with robust error handling
      - name: Get ALB DNS
        id: alb-dns
        run: |
          echo "Attempting to get ALB DNS..."
          # Try multiple methods to get ALB DNS
          DNS_NAME=""
          
          # Method 1: Try terraform output
          if [ -d "iac" ]; then
            cd iac
            DNS_NAME=$(terraform output -raw albDnsName 2>/dev/null || echo "")
            cd ..
          fi
          
          # Method 2: Try AWS CLI if terraform failed
          if [ -z "$DNS_NAME" ]; then
            echo "Falling back to AWS CLI..."
            DNS_NAME=$(aws elbv2 describe-load-balancers \
              --region ${{ env.AWS_REGION }} \
              --names tv-devops-alb \
              --query 'LoadBalancers[0].DNSName' \
              --output text 2>/dev/null || echo "")
          fi
          
          # Method 3: Try generic describe if named lookup failed
          if [ -z "$DNS_NAME" ]; then
            echo "Falling back to generic ALB lookup..."
            DNS_NAME=$(aws elbv2 describe-load-balancers \
              --region ${{ env.AWS_REGION }} \
              --query 'LoadBalancers[?contains(LoadBalancerName,`tv-devops`)].DNSName' \
              --output text 2>/dev/null || echo "")
          fi
          
          if [ -z "$DNS_NAME" ]; then
            echo "ERROR: Could not retrieve ALB DNS name"
            exit 1
          fi
          
          echo "ALB DNS Name: $DNS_NAME"
          echo "ALB_DNS=$DNS_NAME" >> $GITHUB_ENV

      # Step 17: Test endpoint with enhanced retry logic
      - name: Test ALB endpoint
        run: |
          echo "Starting ALB health check..."
          MAX_RETRIES=15
          RETRY_DELAY=20
          HEALTH_ENDPOINT="http://${ALB_DNS}/health"
          
          for ((i=1; i<=$MAX_RETRIES; i++)); do
            echo "Attempt $i/$MAX_RETRIES: Testing $HEALTH_ENDPOINT"
            
            # Use curl with timeout and detailed error reporting
            if curl -sSf --connect-timeout 10 --max-time 15 "$HEALTH_ENDPOINT"; then
              echo "Health check passed successfully"
              exit 0
            else
              CURL_EXIT_CODE=$?
              echo "Health check failed (curl exit code: $CURL_EXIT_CODE)"
              
              # Additional debugging for common curl errors
              case $CURL_EXIT_CODE in
                6) echo "Error: Could not resolve host";;
                7) echo "Error: Failed to connect to host";;
                28) echo "Error: Connection timed out";;
                *) echo "Error: Unknown curl error";;
              esac
              
              if [ $i -lt $MAX_RETRIES ]; then
                echo "Waiting $RETRY_DELAY seconds before next attempt..."
                sleep $RETRY_DELAY
                
                # Optional: Verify ALB state
                echo "Current ALB state:"
                aws elbv2 describe-load-balancers \
                  --region ${{ env.AWS_REGION }} \
                  --names tv-devops-alb \
                  --query 'LoadBalancers[0].State.Code' \
                  --output text || echo "ALB status check failed"
              fi
            fi
          done
          
          echo "ALB health check failed after $MAX_RETRIES attempts"
          echo "Final ALB state:"
          aws elbv2 describe-load-balancers \
            --region ${{ env.AWS_REGION }} \
            --names tv-devops-alb \
            --query 'LoadBalancers[0]' \
            --output json
          exit 1
